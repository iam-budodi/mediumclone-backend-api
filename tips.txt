npm i -g @nestjs/cli : Install nest CLI globally.
nest new project-name : Scaffold a new project
npm i --save @nestjs/core @nestjs/common rxjs reflect-metadata : Manually create a new project from scratch by installing the core and supporting files with npm (or yarn).

nest g resource [name] : Creating a CRUD controller with the validation built-in, using the CLI's CRUD generator.

nest g controller [conrollerName] : Execute to create a controller using the CLI

create modules for every feature and isolate everything related to it inside the module.

relative path are bad:
  - not easy to understand project directory tree
  - cant copy paste them, the import will fail
  - cant replace it everywhere at once as pathes may be different

it is recommended to configure absolute path

postgres commands:
    - sudo psql -U budodi postgres => connect using default postgres user
    - \l => list all our databases
    - \du => list of users/roles
    - create database <dbName>; => creates dbs
    - create user <userName> with encrypted password '<password>'; => create roles
    - grant all privileges on database <dbName> to <userName>; => allow user to manage db
    - \dc <dbName> => connect to the database
    - \dt => display list of all tables
    - \d <tableName> => descibes how the table looks like
    -> \x => enables pretty print
    - INSERT INTO <tableName>(<columnNames separated by comma>) VALUES('<columnValue>'); => create record
    - SELECT * FROM <tableName>; => retrieve data from table

npm install pg typeorm and @nestjs/typeorm to configure and bind postgres to nest app
create ormconfig.ts file on src directory and add config params then export it.

migrations:
  -> you have a database which you just created and so it is empty.
  -> you then want to add something i.e tables.
  -> this is done through migrations i.e we are creating new migration and inside we are creating a table.
  -> this means we are saving somewhere our changes in a database
  -> every single time when we want to update something in our db, we are migrating our schema i.e we have
     the old state and we are getting the new one through migration, this is somewhat similar to git where
     you can switch to older version.
  -> with migration you always know in which state you were and what state will come
  -> synchronize: true from typeorm is unsafe and not recommended for production as we dont want to remove data there
  -> we must fully control how we create tables
  -> it is recommended to use migrations for real world project.

Project flow:
  -> normally we want to start our application or at least setup our project
  -> then create db completely from CLI
  -> we migrate all our migrations, then we have a schema of the tables

we may want to use typeorm outside the node module, to avoid Executing every command in node module's path.
for this we create a new script inside the package.json file, only to use them inside other commands.

change synchronize to false to revoke control from typeorm

typeorm script:
  -> "typeorm": "ts-node -r tsconfig-paths/register ./node_modules/typeorm/cli.js --config src/ormconfig.ts",

migrations scripts:
  -> "db:drop": "npm run typeorm schema:drop" -> npm run db:drop => drops all tables
  -> "db:create": "npm run typeorm migration:generate -- -n" -> npm run db:create <migrationName> => creates new migration
  -> "db:migrate": "npm run typeorm migration:run" -> Execute our migration in the correct order

now with this implementation, when you want to create a new migration, typeorm checks in what state our table and
database is and what entities we have, if we have something different, then this change will go in this migration

migration flow:
  -> first, we are committing all migrationsas they are part of the project, i.e everytime you are changing db
     you are creating new migration.
     1. this is a must
     2. when you want to run your project on new machine, you clone it, install all dependencies then you do
        migrations(i.e run command: npm run db:migrate) only if the db is empty, if not run command: npm run db:drop
        followed by command: npm run db:migrate.

Data Transfer Object (DTO)
  -> post request has a body
  -> this body is named DTO inside nestjs(check nestjs docs on controller section)
  -> it is the schema of the payload that we provide for our backend
  -> it's important for validation inside nestjs
  -> it is a class not an interface
  -> DTOs are used only for payload(i.e request body) otherwise use an interface(for returned object/data)

interface vs class in typescript
  -> interface is just a data type which exist only in typescript
  -> class exist on both js and ts


authentication with jwt
  -> backend generate some token
  -> the token is then saved in the frontend eg. in the cookies
  -> the cookie is then attached to every request
  -> our backend will then know what request is coming and how to consume/respond to the request
  -> the backend can then provide correct or restrict/forbid response if the jwt token is incorrect
  -> use jsonwebtoken to genarate jwt instead of passport.js which is a bit complex


typescript any datatype is bad, it is recommended to create an interface to replace it.

validating requests
  -> use nestjs pipes (check the docs)
  -> we may either want to validate or transform request body from requests
  -> for validation use validationPipe
  -> Pipes do something with parameters before hitting the actions in controllers
  -> install additional packages ie class-validator and class-transformer
  -> use validation decorators inside DTO

middlewares
  -> useful when working with token
  -> frontend will get token and store it in the cookie/local storage and attach it to every request
  -> backend will know what request belong to what user
  -> the token is likely inserted in the request header
  -> almost same as Pipes in way that they do something with request itself before hitting the actions in controllers
  -> are not working with parameters rather request itself
  -> prefered for authentication
  -> the middleware will take request header before request hits controller
  -> from the authorization token in request header, take the current user and attach to the request
  -> allows to get user directly from the request without bothering with token

for slug
  -> use slugify package 